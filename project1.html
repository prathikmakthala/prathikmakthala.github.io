<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>LLM Fine-Tuning for a Medical Chatbot - Prathik Goud Makthala</title>
</head>
<body>

    <!-- Navbar -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top">
        <div class="container">
            <a class="navbar-brand" href="index.html">Prathik Goud Makthala</a>
            <a class="btn btn-outline-light" href="index.html#projects">Back to All Projects</a>
        </div>
    </nav>

    <!-- Project Section -->
    <section class="py-5">
        <div class="container mt-5">
            <div class="row">
                <div class="col-lg-8 mx-auto">
                    <!-- Project Title -->
                    <h1 class="display-4">LLM Fine-Tuning for a Medical Chatbot</h1>
                    <p class="lead text-muted">A deep dive into the machine learning pipeline for a question-answering AI.</p>
                    <hr class="my-4">

                    <!-- Project Image -->
                    <img src="https://source.unsplash.com/random/1200x800/?medical,ai,technology" class="img-fluid rounded mb-4" alt="Medical AI">

                    <!-- Project Overview -->
                    <h3 class="mb-3">Project Overview</h3>
                    <p>This project is a complete machine learning pipeline that demonstrates the process of fine-tuning a `google/flan-t5-base` language model for a specialized question-answering task. The goal was to create a model that could form the core of an intelligent medical chatbot.</p>
                    <p>The process involved several key stages. First, a raw dataset of questions and answers was loaded and meticulously cleaned. The text was then preprocessed using the Natural Language Toolkit (NLTK) for tasks like stopword removal and lemmatization. Following this, exploratory data analysis was performed to understand the characteristics of the dataset. The core of the project involved using the Hugging Face `transformers` library to fine-tune the pre-trained T5 model on the prepared data. Finally, the model's performance was evaluated using the ROUGE metric to measure its ability to generate accurate and relevant answers.</p>

                    <!-- Technologies Used -->
                    <h3 class="mt-5 mb-3">Technologies Used</h3>
                    <span class="badge bg-primary me-2 mb-2">Python</span>
                    <span class="badge bg-primary me-2 mb-2">Hugging Face Transformers</span>
                    <span class="badge bg-primary me-2 mb-2">PyTorch</span>
                    <span class="badge bg-primary me-2 mb-2">Pandas</span>
                    <span class="badge bg-secondary me-2 mb-2">NLTK</span>
                    <span class="badge bg-secondary me-2 mb-2">Scikit-learn</span>
                    <span class="badge bg-secondary me-2 mb-2">Matplotlib</span>

                    <!-- Project Links -->
                    <div class="mt-5 text-center">
                        <a href="https://github.com/prathikmakthala/LLM_chatbot" target="_blank" rel="noopener noreferrer" class="btn btn-lg btn-primary">View on GitHub</a>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer class="py-4 bg-dark text-white text-center">
        <div class="container">
            <p class="mb-0">Copyright &copy; <span id="year"></span> Prathik Goud Makthala. All Rights Reserved.</p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
    <!-- Custom JS -->
    <script src="js/script.js"></script>
</body>
</html>
